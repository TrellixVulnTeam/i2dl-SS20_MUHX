{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization in PyTorch Lightning with Optuna\n",
    "\n",
    "In notebook 4 of this exercise, you've learned how to develop and train models with PyTorch Lightning.\n",
    "\n",
    "In this optional notebook, we'll show you how you can perform hyperparameter optimization in PyTorch Lightning using the  framework *Optuna*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "#!pip install pytorch-lightning==0.7.6 > /dev/null\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from exercise_code.MyPytorchModel import MyPytorchModel\n",
    "from exercise_code.Util import save_model, load_model, test_and_save\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png></img>\n",
    "\n",
    "Optuna is an automatic hyperparameter optimization framework, which works with several deep learning libraries, including PyTorch Lightning. Have a look at https://github.com/optuna/optuna!\n",
    "\n",
    "Two important concepts of Optuna are the terms **Study** and **Trial**:\n",
    "* **Study**: optimization based on an objective function (e.g.: maximize validation accuracy)\n",
    "* **Trial**: a single execution of the objective function (i.e., train a model for a specific hyperparameter configuration)\n",
    "\n",
    "The goal of a study is to find out the optimal set of hyperparameter values through multiple trials (e.g., n_trials=100). Optuna is a framework designed for the automation and the acceleration of the optimization studies.\n",
    "\n",
    "On a high level, hyper parameter tuning with Optuna works very similar to the search algorithms we implemented in exercise 6. However, Optuna has a set of advantages:\n",
    "\n",
    "* **Parallelization** of hyperparameter searches over multiple threads or processes without modifying code\n",
    "* more **efficient search algorithms** for large search spaces and **pruning of unpromising trials** for faster results\n",
    "* many additional features for automated search of optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Optuna\n",
    "\n",
    "If you haven't done yet, install Optuna via pip by uncommenting the line in the cell below: `!pip install optuna`.\n",
    "\n",
    "If you want to install Optuna with conda or you have problems with the installation via pip, you can also use `$ conda install -c conda-forge optuna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n",
    "\n",
    "The default logger in PyTorch Lightning automatically writes to event files to be consumed by TensorBoard. \n",
    "\n",
    "Here we just set up a simple callback, that keeps the metrics from each validation step in memory, which we can then use to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "As mentioned above, the goal of our hyperparameter tuning is, to optimize an objective function by finding the best set of hyperparameters. \n",
    "(\n",
    "Optuna is a black-box optimizer, which means we need to provide this objective function (`objective(trial)`), which gets passed a `trial` object and returns a numerical value to evaluate the performance of the current hyperparameters and where to sample in the upcoming trial.\n",
    "\n",
    "In our case, the metric we want to optimize is the validation accuracy of our models. To get the validation accuracy for a given hyperparameter configuration, we do the same thing as in the previous notebook:\n",
    "\n",
    "* define a PL-trainer\n",
    "* define hyper parameters - by sampling them from the provided hyperparameter ranges, similar as in our old random search implementation\n",
    "* initialize a model with these hyperparameters\n",
    "* train that model, and report its validation accuracy as our objective function value\n",
    "\n",
    "As you can see in the code cell below, the implementation is straight forward:\n",
    "\n",
    "PS: notice the different sampling modes, e.g. `trial.suggest_int`, `trial.suggest_loguniform` which are also similar to our previous implementation! Check out the documentation at https://optuna.readthedocs.io/en/latest/reference/trial.html to find out about further sampling modes, and **feel free to add additional hyperparameters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # as explained above, we'll use this callback to collect the validation accuracies\n",
    "    metrics_callback = MetricsCallback()  \n",
    "    \n",
    "    # create a trainer\n",
    "    trainer = pl.Trainer(\n",
    "        #train_percent_check=1.0,\n",
    "        #val_percent_check=1.0,\n",
    "        logger=False,                                                                  # deactivate PL logging\n",
    "        max_epochs=5,                                                                  # epochs\n",
    "        gpus=0 if torch.cuda.is_available() else None,                                 # #gpus\n",
    "        callbacks=[metrics_callback],                                                  # save latest accuracy\n",
    "        early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=\"val_acc\"), # early stopping\n",
    "    )\n",
    "\n",
    "    # here we sample the hyper params, similar as in our old random search\n",
    "    trial_hparams = {\"n_hidden\": trial.suggest_int(\"n_hidden\", 100, 256), \n",
    "                     \"batch_size\": 64, \n",
    "                     \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-7, 1e-3), \"weight_decay\": \n",
    "                     trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-4)}\n",
    "    \n",
    "    # create model from these hyper params and train it\n",
    "    model = MyPytorchModel(trial_hparams)\n",
    "    model.prepare_data()\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # save model\n",
    "    save_model(model, '{}.p'.format(trial.number), \"checkpoints\")\n",
    "\n",
    "    # return validation accuracy from latest model, as that's what we want to minimize by our hyper param search\n",
    "    return metrics_callback.metrics[-1][\"val_acc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference to our previous search implementation is, that Optuna does not sample randomly! \n",
    "\n",
    "It uses a **Tree-structured Parzen Estimator (TPE)** sampler, which is a kind of bayesian optimization, and more efficient than a pure random search, because it chooses the hyperparameter values after evaluating all previous trials to make a smart guess where the best hyperparameters can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've defined our objective function, we can start the search.\n",
    "\n",
    "For that, we can also provide a **pruner**, which is a very useful concept supported by Optuna: Similar to *early stopping*, a pruner automatically stops unpromising trials in early stages of training (a.k.a. automated early-stopping) and therefore can significantly speed up the search.\n",
    "\n",
    "If you want to specify a pruner, have a look at the available options at https://optuna.readthedocs.io/en/latest/reference/pruners.html. \n",
    "\n",
    "Then, we create our `study`, define the direction that we want to optimize (i.e., *maximize* the validation accuracy) and start the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "WARNING:lightning:No environment variable for node rank defined. Set as 0.\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "-----------------------------------\n",
      "0 | model   | Sequential | 377 K \n",
      "1 | model.0 | Linear     | 362 K \n",
      "2 | model.1 | Dropout    | 0     \n",
      "3 | model.2 | ReLU       | 0     \n",
      "4 | model.3 | Linear     | 14 K  \n",
      "5 | model.4 | Dropout    | 0     \n",
      "6 | model.5 | ReLU       | 0     \n",
      "7 | model.6 | Linear     | 1 K   \n",
      "INFO:lightning:\n",
      "  | Name    | Type       | Params\n",
      "-----------------------------------\n",
      "0 | model   | Sequential | 377 K \n",
      "1 | model.0 | Linear     | 362 K \n",
      "2 | model.1 | Dropout    | 0     \n",
      "3 | model.2 | ReLU       | 0     \n",
      "4 | model.3 | Linear     | 14 K  \n",
      "5 | model.4 | Dropout    | 0     \n",
      "6 | model.5 | ReLU       | 0     \n",
      "7 | model.6 | Linear     | 1 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val-Acc=0.0012\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cefaf59b504e069e65623a198c6cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "INFO:lightning:Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2020-06-12 15:30:15,813] Setting status of trial#0 as TrialState.FAIL because of the following error: IndexError('list index out of range')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/k.karmakar/opt/anaconda3/lib/python3.7/site-packages/optuna/study.py\", line 734, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"<ipython-input-31-4a8ae354f1f3>\", line 31, in objective\n",
      "    return metrics_callback.metrics[-1][\"val_acc\"]\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-eee6b686b824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpruner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNopPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 )\n\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    680\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             message = \"Setting status of trial#{} as {}. {}\".format(\n",
      "\u001b[0;32m<ipython-input-31-4a8ae354f1f3>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# return validation accuracy from latest model, as that's what we want to minimize by our hyper param search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.NopPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=10, timeout=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we save every model in a directory called `./checkpoints`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've finished the search, we access the best trial at `study.best_trial`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 5\n",
      "Best trial:\n",
      "  Value: 0.4832\n",
      "  Params: \n",
      "    n_hidden: 192\n",
      "    learning_rate: 2.175638438219492e-06\n",
      "    weight_decay: 1.4097352211821993e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(best_trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your best model\n",
    "\n",
    "\n",
    "When you're done with your hyperparameter search and have achieved at least 50% validation accuracy, you can save your best model to the `./models`-directory in order to submit the model.\n",
    "\n",
    "Before that, we will check again whether the number of parameters is below 5 Mio and the file size is below 20 MB.\n",
    "\n",
    "When your final model is saved, we'll lastly report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation-Accuracy: 0.4815%\n",
      "That's too low! Please tune your model in order to reach at least 50.0% before running on the test set and submitting!\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model('checkpoints/{}.p'.format(best_trial.number))\n",
    "best_model.prepare_data()\n",
    "test_and_save(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove checkpoints directory\n",
    "Lastly, let's remove the checkpoints directory again to clear up space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Here we provided just a quick overview about hyperparameter optimization using Optuna. Optuna has a lot more to offer, so definitely make sure to check out the following resources:\n",
    "\n",
    "* Source Code: https://github.com/optuna/optuna\n",
    "* Docs: https://optuna.readthedocs.io/en/stable/\n",
    "* Website: https://optuna.org\n",
    "* Paper: https://arxiv.org/pdf/1907.10902.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
